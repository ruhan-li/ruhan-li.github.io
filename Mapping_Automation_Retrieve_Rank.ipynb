{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8606daab",
   "metadata": {},
   "source": [
    "\n",
    "# Mapping Automation: Retrieve → Rank Baseline\n",
    "\n",
    "This notebook suggests `(after_table, after_attribute)` targets in a **data lake** for each input `(before_table, before_attribute)` using:\n",
    "- TF‑IDF **word** vectors (captures tokens)\n",
    "- TF‑IDF **character** n‑grams (robust to `camelCase` / `snake_case` / typos)\n",
    "- Lightweight lexical signals (token **Jaccard**, token **containment**, and **SequenceMatcher** from Python stdlib)\n",
    "\n",
    "> No external packages beyond scikit‑learn are required; `difflib` is part of Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import standard libs and sklearn pieces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher  # stdlib, no install needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517693bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define compact domain synonyms to improve matching on short column names\n",
    "SYN = {\n",
    "    \"qty\": [\"quantity\", \"qty\"],\n",
    "    \"quantity\": [\"quantity\", \"qty\", \"amount\", \"size\"],\n",
    "    \"amt\": [\"amount\", \"amt\", \"notional\"],\n",
    "    \"ccy\": [\"currency\", \"ccy\", \"fx\"],\n",
    "    \"currency\": [\"currency\", \"ccy\", \"fx\"],\n",
    "    \"date\": [\"date\", \"dt\"],\n",
    "    \"dt\": [\"date\", \"dt\"],\n",
    "    \"id\": [\"id\", \"identifier\", \"code\", \"key\"],\n",
    "    \"identifier\": [\"id\", \"identifier\", \"code\", \"key\"],\n",
    "    \"code\": [\"code\", \"id\"],\n",
    "    \"maturity\": [\"maturity\", \"mat\"],\n",
    "    \"product\": [\"product\", \"prd\"],\n",
    "    \"classification\": [\"classification\", \"class\", \"category\", \"type\"],\n",
    "    \"class\": [\"classification\", \"class\", \"category\", \"type\"],\n",
    "    \"type\": [\"type\", \"classification\", \"class\", \"category\"],\n",
    "    \"source\": [\"source\", \"src\", \"origin\"],\n",
    "    \"system\": [\"system\", \"sys\"],\n",
    "    \"position\": [\"position\", \"pos\"],\n",
    "    \"trade\": [\"trade\", \"trx\", \"transaction\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize a string into space-separated tokens\n",
    "def normalize(s: str) -> str:\n",
    "    # split camelCase, then split common separators, then split letters from digits\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', s)\n",
    "    s = re.sub(r\"[_\\-/\\.]\", \" \", s)\n",
    "    s = re.sub(r\"([a-zA-Z])(\\d)\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"(\\d)([a-zA-Z])\", r\"\\1 \\2\", s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenize and apply lightweight synonym expansion\n",
    "def expand_synonyms(tokens):\n",
    "    expanded = []\n",
    "    for t in tokens:\n",
    "        expanded.append(t)\n",
    "        if t in SYN:\n",
    "            for alt in SYN[t]:\n",
    "                if alt not in expanded:\n",
    "                    expanded.append(alt)\n",
    "    return expanded\n",
    "\n",
    "def tokens(s: str):\n",
    "    toks = normalize(s).split()\n",
    "    return expand_synonyms(toks)\n",
    "\n",
    "# small lexical utilities for scoring\n",
    "def seq_ratio(a, b):\n",
    "    # sequence similarity over normalized strings\n",
    "    return SequenceMatcher(None, normalize(a), normalize(b)).ratio()\n",
    "\n",
    "def jaccard(a_list, b_list):\n",
    "    A, B = set(a_list), set(b_list)\n",
    "    if not A and not B:\n",
    "        return 0.0\n",
    "    return len(A & B) / max(1, len(A | B))\n",
    "\n",
    "def containment(a_list, b_list):\n",
    "    # fraction of before tokens found inside the after tokens\n",
    "    A, B = set(a_list), set(b_list)\n",
    "    if not A:\n",
    "        return 0.0\n",
    "    return len(A & B) / len(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set file paths (update these to your environment)\n",
    "hist_path = \"Sample input data.xlsx\"              # optional: historic pairs for vectorizer context\n",
    "after_catalog_path = \"after_catalog_position.csv\" # required: all candidate targets (after_table, after_attribute)\n",
    "before_list_path = \"before_columns_to_map.csv\"    # optional: list of before cols to score in batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read historical pairs if available\n",
    "try:\n",
    "    hist_raw = pd.read_excel(hist_path, sheet_name=0)\n",
    "    hist = hist_raw.rename(columns={\n",
    "        \"Before Table\": \"before_table\",\n",
    "        \"Before Attribute\": \"before_attribute\",\n",
    "        \"After Table\": \"after_table\",\n",
    "        \"After Attribute\": \"after_attribute\",\n",
    "        \"Applicable Products\": \"applicable_products\",\n",
    "        \"Mandatory\": \"mandatory\",\n",
    "    })\n",
    "except Exception:\n",
    "    # proceed without history if file not found\n",
    "    hist = pd.DataFrame(columns=[\"before_table\",\"before_attribute\",\"after_table\",\"after_attribute\",\"applicable_products\",\"mandatory\"])\n",
    "\n",
    "# read the after-catalog (entire candidate universe)\n",
    "after_catalog = pd.read_csv(after_catalog_path)\n",
    "assert {\"after_table\",\"after_attribute\"} <= set(after_catalog.columns), \"after_catalog needs after_table, after_attribute\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901dc559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build textual fields for vectorization\n",
    "after_catalog = after_catalog.copy()\n",
    "after_catalog[\"after_text\"] = after_catalog[\"after_table\"].map(normalize) + \" \" + after_catalog[\"after_attribute\"].map(normalize)\n",
    "\n",
    "if len(hist):\n",
    "    hist[\"before_text\"] = (\n",
    "        hist[\"before_table\"].map(normalize) + \" \" +\n",
    "        hist[\"before_attribute\"].map(normalize) + \" \" +\n",
    "        hist.get(\"applicable_products\", \"\").fillna(\"\").map(normalize).astype(str) + \" \" +\n",
    "        hist.get(\"mandatory\", \"\").fillna(\"\").map(normalize).astype(str)\n",
    "    ).str.strip()\n",
    "    word_corpus = pd.concat([hist[\"before_text\"], after_catalog[\"after_text\"]], ignore_index=True)\n",
    "    char_corpus = pd.concat([hist[\"before_attribute\"].map(normalize), after_catalog[\"after_attribute\"].map(normalize)], ignore_index=True)\n",
    "else:\n",
    "    word_corpus = after_catalog[\"after_text\"]\n",
    "    char_corpus = after_catalog[\"after_attribute\"].map(normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61012b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit word and char vectorizers\n",
    "word_vec = TfidfVectorizer(ngram_range=(1,2), min_df=1).fit(word_corpus)\n",
    "char_vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1).fit(char_corpus)\n",
    "\n",
    "# precompute after-side matrices for fast scoring\n",
    "after_word_mat = word_vec.transform(after_catalog[\"after_text\"]).toarray()\n",
    "after_char_mat = char_vec.transform(after_catalog[\"after_attribute\"].map(normalize)).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22443980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rank candidates for one (before_table, before_attribute) pair\n",
    "def rank_candidates(before_table: str, before_attribute: str, topn: int = 10) -> pd.DataFrame:\n",
    "    before_text = normalize(before_table) + \" \" + normalize(before_attribute)\n",
    "    bw = word_vec.transform([before_text]).toarray()\n",
    "    bc = char_vec.transform([normalize(before_attribute)]).toarray()\n",
    "\n",
    "    sim_word = cosine_similarity(bw, after_word_mat)[0]\n",
    "    sim_char = cosine_similarity(bc, after_char_mat)[0]\n",
    "\n",
    "    b_toks = tokens(before_attribute)\n",
    "\n",
    "    scores = []\n",
    "    for j, cand in after_catalog.iterrows():\n",
    "        a_attr = cand[\"after_attribute\"]\n",
    "        a_toks = tokens(a_attr)\n",
    "        s_seq = seq_ratio(before_attribute, a_attr)\n",
    "        s_jac = jaccard(b_toks, a_toks)\n",
    "        s_cont = containment(b_toks, a_toks)\n",
    "        score = (\n",
    "            0.35 * sim_word[j] +\n",
    "            0.25 * sim_char[j] +\n",
    "            0.15 * s_seq +\n",
    "            0.15 * s_jac +\n",
    "            0.10 * s_cont\n",
    "        )\n",
    "        scores.append(score)\n",
    "\n",
    "    order = np.argsort(-np.array(scores))\n",
    "    out = after_catalog.iloc[order].copy().head(topn)\n",
    "    out[\"score\"] = np.array(scores)[order][:topn]\n",
    "    return out.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example 1: attribute with mixed words\n",
    "rank_candidates(\"position table\", \"TradeDateQuantity\", topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b297593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example 2: attribute with concatenated business terms\n",
    "rank_candidates(\"position table\", \"productclassificationsource\", topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch scoring from a CSV: expects columns [before_table, before_attribute]\n",
    "try:\n",
    "    before_df = pd.read_csv(before_list_path)\n",
    "except Exception:\n",
    "    before_df = pd.DataFrame([\n",
    "        {\"before_table\":\"position table\",\"before_attribute\":\"TradeDateQuantity\"},\n",
    "        {\"before_table\":\"position table\",\"before_attribute\":\"productclassificationsource\"},\n",
    "    ])\n",
    "\n",
    "rows = []\n",
    "for _, r in before_df.iterrows():\n",
    "    bt = r[\"before_table\"]\n",
    "    ba = r[\"before_attribute\"]\n",
    "    preds = rank_candidates(bt, ba, topn=5)\n",
    "    row = {\"before_table\": bt, \"before_attribute\": ba}\n",
    "    for k in range(len(preds)):\n",
    "        row[f\"pred_{k+1}_table\"] = preds.loc[k, \"after_table\"]\n",
    "        row[f\"pred_{k+1}_attribute\"] = preds.loc[k, \"after_attribute\"]\n",
    "        row[f\"pred_{k+1}_score\"] = float(preds.loc[k, \"score\"])\n",
    "    rows.append(row)\n",
    "\n",
    "pred_df = pd.DataFrame(rows)\n",
    "pred_df.to_csv(\"mapping_suggestions.csv\", index=False)\n",
    "pred_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apply a simple decision rule for auto-mapping vs review\n",
    "auto_threshold = 0.40  # adjust based on your tolerance for false positives\n",
    "\n",
    "def auto_map(row):\n",
    "    if pd.notna(row.get(\"pred_1_score\", np.nan)) and row[\"pred_1_score\"] >= auto_threshold:\n",
    "        return \"AUTO_MAP\"\n",
    "    return \"REVIEW\"\n",
    "\n",
    "pred_df[\"decision\"] = pred_df.apply(auto_map, axis=1)\n",
    "pred_df.to_csv(\"mapping_suggestions_with_decisions.csv\", index=False)\n",
    "pred_df.head(10)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
